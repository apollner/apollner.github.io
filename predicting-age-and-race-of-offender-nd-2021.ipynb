{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aronpollner/predicting-age-and-race-of-offender-nd-2021?scriptVersionId=114908006\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# FBI crime report analysis\n##### The objective of this notebook is to use **Machine Learning** models to predict crime data. I chose to predict the age (numerical) and race (categorical) of the offenders based on data from the victims (age, race, ethnicity, sex, etc), the relationship between victim and offender, crime location, crime type, etc. I'll be using North Dakota 2021 data.","metadata":{}},{"cell_type":"markdown","source":"![S](https://www.fbi.gov/image-repository/nibrs-logo-1.jpg/@@images/image/large)\n","metadata":{}},{"cell_type":"markdown","source":"   ![](https://media0.giphy.com/media/rCqHtYuB0a9re731gG/giphy.gif)","metadata":{}},{"cell_type":"markdown","source":"#### Library Imports","metadata":{}},{"cell_type":"code","source":"import itertools\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport re\nfrom tqdm import tqdm\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nimport seaborn as sns\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n# modeling\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor,RandomForestRegressor,\\\nExtraTreesRegressor,BaggingRegressor,RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,\\\n     AdaBoostClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBRegressor\n# data evaluation\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,\\\n     mean_squared_error,r2_score,mean_absolute_error,accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\npd.options.mode.chained_assignment = None \nnp.random.seed(31415)","metadata":{"id":"d49d333a-f2bc-44c7-9cc9-9c9010cbb770","scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-12-27T13:41:25.770193Z","iopub.execute_input":"2022-12-27T13:41:25.770607Z","iopub.status.idle":"2022-12-27T13:41:25.780994Z","shell.execute_reply.started":"2022-12-27T13:41:25.770568Z","shell.execute_reply":"2022-12-27T13:41:25.780115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Loading","metadata":{}},{"cell_type":"code","source":"state = 'ND-'\nyear = '2021'\n\nfile_path = rf'/kaggle/input/us-fbi-nibrs-crime-data-2021-all-states/{state}{year}/'\n# list all the files from the directory\nfile_list = os.listdir(file_path)\ncsv_list = []\n# list only csv files\nfor file in file_list:\n    if re.search('.+csv', file):\n        csv_list.append(file)\n# dictionary of dataframes corresponding to each csv file, where the key is the\n# dataframe name and the value is the actual data frame\ndf_dict = {file[:-4].lower(): pd.read_csv(rf'/kaggle/input/us-fbi-nibrs-crime-data-2021-all-states/{state}{year}/{file}')\n           for file in csv_list}\n# Since we chose the state and year, data_year and state columns don't give any extra information\n# so we drop them\nfor key, val in df_dict.items():\n    val=val[val.columns.drop(list(val.filter(regex='data_year')))].copy()\n    val=val[val.columns.drop(list(val.filter(regex='state')))].copy()\n    df_dict[key]=val\n# Now we create variables for all the dictionary keys and assign to them\n# the value from the respective value of the dictionary according to their key\nlocals().update(df_dict)","metadata":{"id":"baa02d6d-fe32-4bab-9c59-b6f38a430c92","execution":{"iopub.status.busy":"2022-12-27T13:41:25.783454Z","iopub.execute_input":"2022-12-27T13:41:25.78394Z","iopub.status.idle":"2022-12-27T13:41:27.009303Z","shell.execute_reply.started":"2022-12-27T13:41:25.783896Z","shell.execute_reply":"2022-12-27T13:41:27.008252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Merging the dataframes\n###### For this project I chose to use the offenders data and ignore the arrestees since the former has more data, although caution should be taken that data collected for offenders that where not arrested might be less reliable","metadata":{}},{"cell_type":"code","source":"df_combined = pd.merge(nibrs_victim_offense, nibrs_offense,\n                       how='outer', on='offense_id')\ndf_combined = pd.merge(df_combined, nibrs_suspect_using,\n                       how='outer', on='offense_id')\ndf_combined = pd.merge(df_combined, nibrs_bias_motivation,\n                       how='outer', on='offense_id')\nnibrs_offender = pd.merge(nibrs_offender, nibrs_age,\n                       how='outer',\n                          on='age_id')\nnibrs_offender = pd.merge(nibrs_offender, ref_race,\n                          on='race_id')\nnibrs_offender = pd.merge(nibrs_offender, nibrs_ethnicity,\n                          on='ethnicity_id')\nnibrs_victim = pd.merge(nibrs_victim, nibrs_age,\n                       how='outer',\n                        on='age_id')\nnibrs_victim = pd.merge(nibrs_victim, ref_race, on='race_id')\nnibrs_victim = pd.merge(nibrs_victim, nibrs_ethnicity,\n                        on='ethnicity_id')\ndf_victim_offender = pd.merge(nibrs_victim, nibrs_offender,\n                              on='incident_id', how='outer', suffixes=('_victim', '_offender'))\ndf_combined = pd.merge(df_combined, df_victim_offender,\n                       how='outer', left_on=['incident_id', 'victim_id'], right_on=['incident_id',\n                                                                                    'victim_id'])\ndf_combined = pd.merge(df_combined, nibrs_incident,\n                       how='outer', on='incident_id')\ndf_combined = pd.merge(df_combined, nibrs_property,\n                       how='outer', on='incident_id')\ndf_combined = pd.merge(df_combined, nibrs_property_desc,\n                       how='outer', on='property_id')\ndf_combined = pd.merge(df_combined, nibrs_victim_offender_rel, how='outer', left_on=[\n    'victim_id', 'offender_id'], right_on=[\n        'victim_id', 'offender_id'])\ndf_combined = pd.merge(df_combined, nibrs_criminal_act,\n                       how='outer', on='offense_id')\ndf_combined = pd.merge(df_combined, nibrs_weapon,\n                       how='outer', on='offense_id')\ndf_combined = pd.merge(df_combined, nibrs_victim_injury,\n                       how='outer', on='victim_id')\ndf_combined = pd.merge(df_combined, agencies,\n                       how='outer', on='agency_id')\ndf_combined = pd.merge(df_combined, nibrs_offense_type, how='outer',\n                       on='offense_code')\ndf_combined = pd.merge(df_combined, nibrs_prop_desc_type,\n                       how='outer', on='prop_desc_id')\ndf_combined = pd.merge(df_combined, nibrs_location_type,\n                       how='outer', on='location_id')\ndf_combined = pd.merge(df_combined, nibrs_bias_list,\n                       how='outer', on='bias_id')\ndf_combined = pd.merge(df_combined, nibrs_weapon_type,\n                       how='outer', on='weapon_id')\ndf_combined = pd.merge(df_combined, nibrs_relationship,\n                       how='outer', on='relationship_id')\ndf_combined = pd.merge(df_combined, nibrs_victim_circumstances,\n                       how='outer', on='victim_id')\ndf_combined = pd.merge(df_combined, nibrs_circumstances,\n                       how='outer', on='circumstances_id')\ndf_combined = pd.merge(df_combined, nibrs_criminal_act_type,\n                       how='outer', on='criminal_act_id')\ndf_combined = pd.merge(df_combined, nibrs_activity_type,\n                       how='outer', on='activity_type_id')\ndf_combined = pd.merge(df_combined, nibrs_victim_type,\n                       how='outer', on='victim_type_id')\ndf_combined = pd.merge(df_combined, nibrs_prop_loss_type,\n                       how='outer', on='prop_loss_id')\ndf_combined = pd.merge(df_combined, nibrs_injury,\n                       how='outer', on='injury_id')\ndf_combined = pd.merge(df_combined, nibrs_using_list, on='suspect_using_id')","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:27.010714Z","iopub.execute_input":"2022-12-27T13:41:27.011115Z","iopub.status.idle":"2022-12-27T13:41:33.854578Z","shell.execute_reply.started":"2022-12-27T13:41:27.011083Z","shell.execute_reply":"2022-12-27T13:41:33.853197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data preprocessing, cleaning and feature engineering\n<a id='Data preprocessing, cleaning and feature engineering'></a>","metadata":{}},{"cell_type":"code","source":"# we create a copy of the combined dataframe to work on\ndf_cleaned=df_combined.copy()","metadata":{"id":"TdpPz0h-9K-T","execution":{"iopub.status.busy":"2022-12-27T13:41:33.857862Z","iopub.execute_input":"2022-12-27T13:41:33.858282Z","iopub.status.idle":"2022-12-27T13:41:34.462808Z","shell.execute_reply.started":"2022-12-27T13:41:33.858246Z","shell.execute_reply":"2022-12-27T13:41:34.461489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the complete dataframe\npd.set_option('display.max_columns', None)\ndf_cleaned","metadata":{"id":"2792b51e-8366-474e-abf7-bc4e70b430ce","outputId":"2b7dcb96-b690-409d-d437-3b4c3b902ac3","execution":{"iopub.status.busy":"2022-12-27T13:41:34.4669Z","iopub.execute_input":"2022-12-27T13:41:34.467336Z","iopub.status.idle":"2022-12-27T13:41:34.696566Z","shell.execute_reply.started":"2022-12-27T13:41:34.467292Z","shell.execute_reply":"2022-12-27T13:41:34.695311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform date to datetime format\ndf_cleaned['incident_date'] = pd.to_datetime(\n    df_cleaned['incident_date'], format=\"%Y/%m/%d\")","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:34.698307Z","iopub.execute_input":"2022-12-27T13:41:34.699979Z","iopub.status.idle":"2022-12-27T13:41:34.936911Z","shell.execute_reply.started":"2022-12-27T13:41:34.699934Z","shell.execute_reply":"2022-12-27T13:41:34.935216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since crime is done by humans and we know that the date has an effect on human behavior, \n# we do some feature engineering extracting new variables from the date data could be useful\n# for our predictions. We extract day of the week,if it is weekend or a holiday, month,\n# day of the month, and week of the year\ndf_cleaned['incident_day'] = df_cleaned['incident_date'].dt.dayofweek\ndf_cleaned['incident_isweekend'] = df_cleaned['incident_day'] > 4\ncal = USFederalHolidayCalendar()\nholidays = cal.holidays()\ndf_cleaned['incident_is_holiday'] = df_cleaned['incident_date'].isin(holidays)\ndf_cleaned['incident_month'] = df_cleaned['incident_date'].dt.month\ndf_cleaned['incident_dayofmonth'] = df_cleaned['incident_date'].dt.day\ndf_cleaned['incident_weekofyear'] =df_cleaned['incident_date'].dt.isocalendar().week","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:34.939097Z","iopub.execute_input":"2022-12-27T13:41:34.9397Z","iopub.status.idle":"2022-12-27T13:41:35.145494Z","shell.execute_reply.started":"2022-12-27T13:41:34.939658Z","shell.execute_reply":"2022-12-27T13:41:35.144246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Each NIBRS offense belongs to one of three categories: Crimes Against Persons, Crimes Against Property, and Crimes Against Society ([See this document](Crimes_Against_Persons_Property_and_Society.pdf)). Therefore we split the data into these categories. In this project will focus on Crimes Against Persons.","metadata":{}},{"cell_type":"code","source":"grouped = df_cleaned.groupby(df_cleaned.crime_against)\ndf_person = grouped.get_group(\"Person\")\ndf_society = grouped.get_group(\"Society\")\ndf_property = grouped.get_group(\"Property\")","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:35.146977Z","iopub.execute_input":"2022-12-27T13:41:35.147336Z","iopub.status.idle":"2022-12-27T13:41:35.378672Z","shell.execute_reply.started":"2022-12-27T13:41:35.147304Z","shell.execute_reply":"2022-12-27T13:41:35.377341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Let's plot the crime against persons along time","metadata":{}},{"cell_type":"markdown","source":"###### First we plot the number of incidents per day along the year. In the background we added green lines representing weekends and orange representing holidays. Here we  can see is that there is a long up-and-down movement few times a year, short weekly movements 52 times a year, and perhaps others.","metadata":{}},{"cell_type":"code","source":"weekends=sorted(df_person[df_person['incident_isweekend']==True].incident_date.unique())\nholidays=sorted(df_person[df_person['incident_is_holiday']==True].incident_date.unique())\ndf_person.groupby('incident_date')['incident_id'].nunique().plot(title='incident_date'+' time series',figsize=(12,6))\ni = 0\nwhile i < len(weekends)-1:\n    plt.axvspan(weekends[i], weekends[i+1]+np.timedelta64(1,'D'), facecolor='green', edgecolor='none', alpha=.2)\n    i += 2\nfor i in range(len(holidays)):\n    plt.axvspan(holidays[i], holidays[i]+np.timedelta64(1,'D'), facecolor='red', edgecolor='none', alpha=.5)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:35.380222Z","iopub.execute_input":"2022-12-27T13:41:35.38063Z","iopub.status.idle":"2022-12-27T13:41:36.222992Z","shell.execute_reply.started":"2022-12-27T13:41:35.380597Z","shell.execute_reply":"2022-12-27T13:41:36.221939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### It can be seen that in general there is more crime during the weekend","metadata":{}},{"cell_type":"code","source":"p1=sns.cubehelix_palette(7)\nplt.figure(figsize=(13,8))\nsns.countplot(x=\"incident_day\", data=df_person,palette=p1)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:36.228171Z","iopub.execute_input":"2022-12-27T13:41:36.229022Z","iopub.status.idle":"2022-12-27T13:41:36.481173Z","shell.execute_reply.started":"2022-12-27T13:41:36.22898Z","shell.execute_reply":"2022-12-27T13:41:36.480225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Here we see something strange, that at 12am there is much more crimes. This might be that crimes without a recorded time are recorded at 12am by default. We'll have to take care of this anomaly.","metadata":{}},{"cell_type":"code","source":"p2=sns.cubehelix_palette(24)\nplt.figure(figsize=(13,8))\nsns.countplot(x=\"incident_hour\", data=df_person, color='grey',palette=p2)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:36.482354Z","iopub.execute_input":"2022-12-27T13:41:36.482829Z","iopub.status.idle":"2022-12-27T13:41:36.88285Z","shell.execute_reply.started":"2022-12-27T13:41:36.482783Z","shell.execute_reply":"2022-12-27T13:41:36.881543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### It seems that theres a trend for crimes to go down along the month, although not all month have 31 days.","metadata":{}},{"cell_type":"code","source":"p3=sns.cubehelix_palette(31)\nplt.figure(figsize=(13,8))\nsns.countplot(x=\"incident_dayofmonth\", data=df_person, color='grey',palette=p3)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:36.884247Z","iopub.execute_input":"2022-12-27T13:41:36.884615Z","iopub.status.idle":"2022-12-27T13:41:37.291493Z","shell.execute_reply.started":"2022-12-27T13:41:36.884582Z","shell.execute_reply":"2022-12-27T13:41:37.289794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Let's remove the columns with only null values\n","metadata":{"id":"66908c30-28b9-46d9-8fc8-39a091307f68"}},{"cell_type":"code","source":"for df in [df_person,df_property,df_society]:\n    df.dropna(thresh=2, axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:37.292983Z","iopub.execute_input":"2022-12-27T13:41:37.293373Z","iopub.status.idle":"2022-12-27T13:41:38.009349Z","shell.execute_reply.started":"2022-12-27T13:41:37.29334Z","shell.execute_reply":"2022-12-27T13:41:38.008092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_person.relationship_name.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:38.010924Z","iopub.execute_input":"2022-12-27T13:41:38.011362Z","iopub.status.idle":"2022-12-27T13:41:38.022107Z","shell.execute_reply.started":"2022-12-27T13:41:38.011239Z","shell.execute_reply":"2022-12-27T13:41:38.020834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Remove all columns where there is a class that represents more than 90% of the data","metadata":{}},{"cell_type":"code","source":"# Remove all columns where there is a class that represents more than 90% of the data\nfor df in [df_person,df_property,df_society]:\n    very_imbalanced_columns=[]\n    for col in tqdm(df.columns):\n        if df[col].value_counts(normalize=True,dropna=False).max() >= 0.90:\n            very_imbalanced_columns.append(col)\n    df.drop(very_imbalanced_columns, axis=1,inplace=True)  ","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:38.024347Z","iopub.execute_input":"2022-12-27T13:41:38.024895Z","iopub.status.idle":"2022-12-27T13:41:39.236821Z","shell.execute_reply.started":"2022-12-27T13:41:38.024846Z","shell.execute_reply":"2022-12-27T13:41:39.235347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Remove all columns where there is more than 90% null values","metadata":{}},{"cell_type":"code","source":"for df in [df_person,df_property,df_society]:\n    # Remove all columns where there is more than 90% null values\n    to_remove=[]\n    for column,null_percent in enumerate((df.isna().sum()/df.shape[0]).to_frame().values):\n        if null_percent >0.90:\n            to_remove.append(column)\n            #print(null_percent,column)\n    df.drop(columns=df.columns[to_remove], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:39.238059Z","iopub.execute_input":"2022-12-27T13:41:39.238461Z","iopub.status.idle":"2022-12-27T13:41:39.60664Z","shell.execute_reply.started":"2022-12-27T13:41:39.238428Z","shell.execute_reply":"2022-12-27T13:41:39.605613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Let's remove columns that seem irrelevant for our goal","metadata":{"id":"TYxW0hG7zjj1"}},{"cell_type":"code","source":"for df in [df_person,df_property,df_society]:\n    df.drop(columns=['num_premises_entered',\n                          'age_range_low_num_victim',\n                         'notes_victim'\n                         ,'population_group_desc','county_name',\n                         'age_range_low_num_offender',\n                         'notes_offender', 'submission_date',\n                         'cleared_except_date','offense_category_name',\n                         'incident_status', 'did','suspected_drug_name','drug_measure_name',\n                         'date_recovered', 'ori', 'legacy_ori',\n                         'ucr_agency_name', 'ncic_agency_name',\n                         'pub_agency_unit','prop_loss_desc',\n                         'suburban_area_flag', 'parent_pop_group_desc',\n                         'mip_flag', 'pe_reported_flag',\n                         'nibrs_start_date', 'nibrs_leoka_start_date',\n                         'nibrs_ct_start_date', 'msa_name', 'ct_flag',\n                         'age_range_high_num', 'est_drug_qty',\n                         'male_officer','male_civilian','criminal_act_desc',\n                         'male_officer+male_civilian', 'female_officer',\n                         'female_civilian', 'female_officer+female_civilian',\n                         'officer_rate', 'hc_flag', 'shr_flag', 'employee_rate',\n                         ], inplace=True, axis=1,errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:39.607995Z","iopub.execute_input":"2022-12-27T13:41:39.608974Z","iopub.status.idle":"2022-12-27T13:41:39.67672Z","shell.execute_reply.started":"2022-12-27T13:41:39.608934Z","shell.execute_reply":"2022-12-27T13:41:39.675088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Looking at nibrs_age csv we see that there are a minority of ages that don't correspond to a specific age. We'll remove this for simplicity","metadata":{}},{"cell_type":"code","source":"df_person=df_person.loc[~df_person['age_id_victim'].isin([1,2,3,103,104])]\ndf_person=df_person.loc[~df_person['age_id_offender'].isin([1,2,3,103,104])]","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:39.678417Z","iopub.execute_input":"2022-12-27T13:41:39.679525Z","iopub.status.idle":"2022-12-27T13:41:39.715418Z","shell.execute_reply.started":"2022-12-27T13:41:39.679479Z","shell.execute_reply":"2022-12-27T13:41:39.714109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Let's remove columns that don't give valuable information like id and code columns","metadata":{}},{"cell_type":"code","source":"for df in [df_person,df_property,df_society]:\n    (df.drop(columns=[i for i in list(df.filter(regex='_id'))],errors='ignore',inplace=True))\n    (df.drop(columns=[i for i in list(df.filter(regex='_code')) if i!='resident_status_code'],errors='ignore',inplace=True))","metadata":{"id":"m1HermoIyj3F","execution":{"iopub.status.busy":"2022-12-27T13:41:39.717064Z","iopub.execute_input":"2022-12-27T13:41:39.717577Z","iopub.status.idle":"2022-12-27T13:41:39.831125Z","shell.execute_reply.started":"2022-12-27T13:41:39.717529Z","shell.execute_reply":"2022-12-27T13:41:39.829832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Let's transform age to numeric","metadata":{}},{"cell_type":"code","source":"df_person[\"age_num_victim\"] = pd.to_numeric(df_person[\"age_num_victim\"])\ndf_person[\"age_num_offender\"] = pd.to_numeric(df_person[\"age_num_offender\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:39.832542Z","iopub.execute_input":"2022-12-27T13:41:39.833376Z","iopub.status.idle":"2022-12-27T13:41:39.862919Z","shell.execute_reply.started":"2022-12-27T13:41:39.833339Z","shell.execute_reply":"2022-12-27T13:41:39.861446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_person.age_num_offender.hist(alpha=.5,label='Age Offender')\ndf_person.age_num_victim.hist(alpha=.5,label='Age Victim')\nplt.title('Histogram Age Victim vs Offender')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:39.864619Z","iopub.execute_input":"2022-12-27T13:41:39.865016Z","iopub.status.idle":"2022-12-27T13:41:40.140732Z","shell.execute_reply.started":"2022-12-27T13:41:39.864985Z","shell.execute_reply":"2022-12-27T13:41:40.139496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predictions","metadata":{}},{"cell_type":"markdown","source":"###### We'll choose the relevant inputs and we'll predict two outputs, one numerical (age of the offender) and one categorical (race of the offender)","metadata":{}},{"cell_type":"code","source":"inputFeature = df_person[['victim_seq_num','age_num_victim','resident_status_code','race_desc_victim',\n'ethnicity_name_victim','incident_hour','pub_agency_name','offense_name','location_name','weapon_name','population'\n,'injury_name','incident_day','incident_isweekend','incident_month','incident_dayofmonth','incident_weekofyear','relationship_name']]\nnumerical_output = df_person[['age_num_offender']]\ncategorial_output = df_person[['race_desc_offender']]","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:40.142582Z","iopub.execute_input":"2022-12-27T13:41:40.142976Z","iopub.status.idle":"2022-12-27T13:41:40.154987Z","shell.execute_reply.started":"2022-12-27T13:41:40.142943Z","shell.execute_reply":"2022-12-27T13:41:40.153635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Categorical predictions","metadata":{}},{"cell_type":"markdown","source":"###### We split the data into 90/10 train/test and shuffle","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(inputFeature, categorial_output, test_size=0.1, random_state=42,stratify=categorial_output)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:40.156995Z","iopub.execute_input":"2022-12-27T13:41:40.157438Z","iopub.status.idle":"2022-12-27T13:41:40.240474Z","shell.execute_reply.started":"2022-12-27T13:41:40.157405Z","shell.execute_reply":"2022-12-27T13:41:40.238616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features=['population','victim_seq_num','age_num_victim','incident_hour','incident_month','incident_day','incident_dayofmonth','incident_weekofyear']\ncategorical_features = ['resident_status_code','race_desc_victim',\n'ethnicity_name_victim','pub_agency_name','offense_name','location_name','weapon_name'\n,'injury_name','relationship_name','incident_isweekend']","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:40.242331Z","iopub.execute_input":"2022-12-27T13:41:40.24271Z","iopub.status.idle":"2022-12-27T13:41:40.249816Z","shell.execute_reply.started":"2022-12-27T13:41:40.242677Z","shell.execute_reply":"2022-12-27T13:41:40.24822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### We create a pipeline for imputing missing values, scaling the numerical features and do one hot encoding for categorical ones","metadata":{}},{"cell_type":"code","source":"numeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:40.251321Z","iopub.execute_input":"2022-12-27T13:41:40.251844Z","iopub.status.idle":"2022-12-27T13:41:40.261421Z","shell.execute_reply.started":"2022-12-27T13:41:40.251796Z","shell.execute_reply":"2022-12-27T13:41:40.259827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)])","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:40.263044Z","iopub.execute_input":"2022-12-27T13:41:40.263448Z","iopub.status.idle":"2022-12-27T13:41:40.272013Z","shell.execute_reply.started":"2022-12-27T13:41:40.263405Z","shell.execute_reply":"2022-12-27T13:41:40.270686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### We see below that all the predictors perform better than predicting the the most common value, with **Random Forest Classifier** and **Extra Trees Classifier** being the best ones. We seee that both classifiers do a (similiar) good job even with imbalanced classes. With a little bit of misclassification between white, african american and american indian or alaska native. We show the accuracy along the confusion matrix and the distribution plot for each model.","metadata":{}},{"cell_type":"code","source":"classifiers = [\n    RandomForestClassifier(),\n    ExtraTreesClassifier(),\n    KNeighborsClassifier(3),\n    DecisionTreeClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    ]\nrace_list=df_person.race_desc_offender.value_counts().index.tolist()\nfor classifier in classifiers:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', classifier)])\n    pipe.fit(X_train, y_train.values.ravel())   \n    print(type(classifier).__name__)\n    print(\"model Accuracy score: %.3f\" % pipe.score(X_test, y_test))\n    confusion_matrix_race = confusion_matrix(y_test, pipe.predict(X_test), labels=race_list)\n    # Plot normalized confusion matrix in race\n    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_race,\n                              display_labels=race_list)\n    a4_dims = (10, 6)\n    fig, ax =plt.subplots(1,2,figsize=a4_dims)\n    disp.plot(xticks_rotation='vertical', ax=ax[0]) \n    sns.countplot(pipe.predict(X_test), ax=ax[1]).set(title=type(classifier).__name__)\n    ax[1].tick_params(axis='x', rotation=90)\n    fig.figure.subplots_adjust(wspace=.3)\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:41:40.273638Z","iopub.execute_input":"2022-12-27T13:41:40.274101Z","iopub.status.idle":"2022-12-27T13:45:27.689983Z","shell.execute_reply.started":"2022-12-27T13:41:40.274064Z","shell.execute_reply":"2022-12-27T13:45:27.688635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### We get the base case which is always prediciting the most common value and see it is the worst","metadata":{}},{"cell_type":"code","source":"race_count = df_person['race_desc_offender'].value_counts()\nrace_base = race_count.idxmax()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:45:27.696361Z","iopub.execute_input":"2022-12-27T13:45:27.696816Z","iopub.status.idle":"2022-12-27T13:45:27.704524Z","shell.execute_reply.started":"2022-12-27T13:45:27.696782Z","shell.execute_reply":"2022-12-27T13:45:27.703283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"model Accuracy score: %.3f\" % accuracy_score(y_test,[race_base]*len(y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:45:27.706253Z","iopub.execute_input":"2022-12-27T13:45:27.706806Z","iopub.status.idle":"2022-12-27T13:45:27.725062Z","shell.execute_reply.started":"2022-12-27T13:45:27.706606Z","shell.execute_reply":"2022-12-27T13:45:27.723208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Numerical Predictions","metadata":{}},{"cell_type":"markdown","source":"###### We split the data into 90/10 train/test and shuffle","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(inputFeature, numerical_output, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:45:27.727554Z","iopub.execute_input":"2022-12-27T13:45:27.728063Z","iopub.status.idle":"2022-12-27T13:45:27.747785Z","shell.execute_reply.started":"2022-12-27T13:45:27.728016Z","shell.execute_reply":"2022-12-27T13:45:27.745855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### We compare the R2 score, RMSE and density plots of the predicting models with the test. It seems that **Random Forest Regressor** and **Extra Trees Regressor** are the best ones","metadata":{}},{"cell_type":"code","source":"regressors = [\nGradientBoostingRegressor()\n,AdaBoostRegressor()\n,ExtraTreesRegressor()\n,BaggingRegressor()\n,RandomForestRegressor()\n,XGBRegressor(objective=\"reg:squarederror\", random_state=42,verbosity=0)\n    ]\n   \nfor regressor in regressors:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('regressor', regressor)])\n    pipe.fit(X_train, y_train.values.ravel())   \n    print(type(regressor).__name__)\n    print(\"model R2 score: %.3f\" % pipe.score(X_test, y_test))\n    print(\"model RMSE: %.3f\" % mean_squared_error(y_test.values.ravel(),pipe.predict(X_test),squared=False))\n    sns.kdeplot(pipe.predict(X_test),label=type(regressor).__name__)\nsns.kdeplot(y_test.values.ravel(),label='Test')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:45:27.751463Z","iopub.execute_input":"2022-12-27T13:45:27.753111Z","iopub.status.idle":"2022-12-27T13:51:22.716801Z","shell.execute_reply.started":"2022-12-27T13:45:27.753053Z","shell.execute_reply":"2022-12-27T13:51:22.715646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### We get the base case which is always prediciting the mean and see it is the worst","metadata":{}},{"cell_type":"code","source":"age_base = df_person['age_num_offender'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:51:22.718708Z","iopub.execute_input":"2022-12-27T13:51:22.719184Z","iopub.status.idle":"2022-12-27T13:51:22.724699Z","shell.execute_reply.started":"2022-12-27T13:51:22.71914Z","shell.execute_reply":"2022-12-27T13:51:22.723764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"base model R2 score: %.3f\" % r2_score(y_test,[age_base]*len(y_test)))\nprint(\"base model RMSE: %.3f\" % mean_squared_error(y_test.values.ravel(),[age_base]*len(y_test),squared=False))","metadata":{"execution":{"iopub.status.busy":"2022-12-27T13:51:22.726731Z","iopub.execute_input":"2022-12-27T13:51:22.727463Z","iopub.status.idle":"2022-12-27T13:51:22.741025Z","shell.execute_reply.started":"2022-12-27T13:51:22.727426Z","shell.execute_reply":"2022-12-27T13:51:22.739811Z"},"trusted":true},"execution_count":null,"outputs":[]}]}